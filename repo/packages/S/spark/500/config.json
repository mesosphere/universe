{
  "type": "object",
  "properties": {
    "service": {
      "description": "DC/OS Spark configuration properties",
      "type": "object",
      "properties": {
        "name": {
          "default": "spark",
          "description": "The Spark Dispatcher will register with Mesos with this as a framework name.  This service will be available at https://<dcos-url>/service/<name>/",
          "type": "string"
        },
        "cpus": {
          "default": 1,
          "description": "CPU shares",
          "minimum": 0.0,
          "type": "number"
        },
        "mem": {
          "default": 1024.0,
          "description": "Memory (MB)",
          "minimum": 1024.0,
          "type": "number"
        },
        "role": {
          "description": "The Spark Dispatcher will register with Mesos with this role.",
          "type": "string",
          "default": "*"
        },
        "service_account": {
          "description": "The Spark Dispatcher will register with Mesos with this principal.",
          "type": "string",
          "default": ""
        },
        "service_account_secret": {
          "description": "The Spark Dispatcher will register with mesos with this secret.",
          "type": "string",
          "default": ""
        },
        "user": {
          "description": "Executors will run as this user.",
          "type": "string",
          "default": "nobody"
        },
        "docker-image": {
          "type": "string",
          "description": "The docker image used to run the dispatcher, drivers, and executors. If no image is specified mesosphere/spark:2.6.0-2.3.2-hadoop-2.7 will be used. If, for example, you need a Spark built with a specific Hadoop version, set this variable to one of the images here: https://hub.docker.com/r/mesosphere/spark/tags/",
          "default": ""
        },
        "log-level": {
          "type": "string",
          "description": "The log level for the Spark Dispatcher.",
          "default": "INFO",
          "enum": [
            "OFF",
            "FATAL",
            "ERROR",
            "WARN",
            "INFO",
            "DEBUG",
            "TRACE",
            "ALL"
          ]
        },
        "virtual_network_enabled": {
          "description": "Enable virtual networking",
          "type": "boolean",
          "default": false
        },
        "virtual_network_name": {
          "description": "The name of the virtual network to join",
          "type": "string",
          "default": "dcos"
        },
        "virtual_network_plugin_labels": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "key": {
                "type": "string",
                "default": ""
              },
              "value": {
                "type": "string",
                "default": ""
              }
            }
          },
          "description": "Labels to pass to the virtual network plugin (e.g., [{\"key\": \"key_1\", \"value\": \"value_1\"}, {\"key\": \"key_2\", \"value\": \"value_2\"}])",
          "default": []
        },
        "spark-history-server-url": {
          "type": "string",
          "description": "URL of The Spark History Server (e.g. https://<dcos-url>/service/spark-history )"
        },
        "UCR_containerizer": {
          "type": "boolean",
          "description": "Launch the Dispatcher using the Universal Container Runtime (UCR)",
          "default": false
        },
        "docker_user": {
          "type": "string",
          "description": "Specify the integer UID for the Linux user, overriding service.user, when running the Spark Driver and Executor containers with the Docker Engine. Directly translates to docker run '--user' parameter. Note: This should typically be set to 99 when running as nobody (default) on RHEL/CentOS.",
          "default": ""
        },
        "use_bootstrap_for_IP_detect": {
          "type": "boolean",
          "description": "Use the bootstrap utility for detecting host IP as opposed to using Spark's internal mechanism, see troubleshooting.md.",
          "default": false
        }
      }
    },
    "security": {
      "description": "Spark security configuration properties",
      "type": "object",
      "properties": {
        "kerberos": {
          "description": "Spark Kerberos configuration.",
          "type": "object",
          "properties": {
            "enabled": {
              "description": "Enable kerberos authentication.",
              "type": "boolean",
              "default": false
            },
            "kdc": {
              "description": "KDC settings for Kerberos",
              "type": "object",
              "properties": {
                "hostname": {
                  "type": "string",
                  "description": "The name or address of a host running a KDC for the realm."
                },
                "port": {
                  "type": "integer",
                  "description": "The port of the host running a KDC for that realm."
                }
              }
            },
            "realm": {
              "type": "string",
              "description": "The Kerberos realm used to render the principal."
            },
            "krb5conf": {
              "description": "Base64 encoded krb5.conf file.  Copied to the drivers and executors so that they may access your KDC. Providing this will override above settings.",
              "type": "string",
              "media": {
                "binaryEncoding": "base64",
                "type": "application/x-yaml"
              },
              "default": ""
            }
          }
        }
      }
    },
    "hdfs": {
      "description": "Spark-HDFS configuration properties",
      "type": "object",
      "properties": {
        "config-url": {
          "type": "string",
          "description": "Base URL that serves HDFS config files (hdfs-site.xml, core-site.xml).  If not set, DC/OS Spark will use its default configuration and read from DC/OS HDFS."
        }
      }
    }
  }
}
